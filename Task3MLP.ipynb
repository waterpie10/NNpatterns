{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Multi Layer Perceptron\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training images shape: (60000, 28, 28)\n",
      "Original training labels shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Check the shape of the data\n",
    "print(\"Original training images shape:\", train_images.shape)\n",
    "print(\"Original training labels shape:\", train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training images shape: (60000, 28, 28)\n",
      "Reshaped training images shape: (60000, 784)\n",
      "Reshaped test images shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# --- Preprocess the Data ---\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "# Convert to float32 first to ensure the division results in a float, not an integer.\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the images from (28, 28) to a flat vector of 784 elements\n",
    "train_images_flat = train_images.reshape(train_images.shape[0], 784)\n",
    "test_images_flat = test_images.reshape(test_images.shape[0], 784)\n",
    "\n",
    "# Check the new shape to confirm it's correct\n",
    "print(f\"Original training images shape: {train_images.shape}\")\n",
    "print(f\"Reshaped training images shape: {train_images_flat.shape}\")\n",
    "print(f\"Reshaped test images shape: {test_images_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training labels shape: (60000,)\n",
      "One-hot encoded training labels shape: (60000, 10)\n",
      "\n",
      "--- Example ---\n",
      "Original first label: 5\n",
      "One-hot encoded first label: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# --- One-Hot Encode the Labels ---\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels_one_hot = to_categorical(train_labels, num_classes=10)\n",
    "test_labels_one_hot = to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "# Check the new shape of the labels and look at an example\n",
    "print(\"Original training labels shape:\", train_labels.shape)\n",
    "print(\"One-hot encoded training labels shape:\", train_labels_one_hot.shape)\n",
    "\n",
    "print(\"\\n--- Example ---\")\n",
    "print(\"Original first label:\", train_labels[0])\n",
    "print(\"One-hot encoded first label:\", train_labels_one_hot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MLP Parameters ---\n",
      "Input->Hidden Weights shape: (784, 20)\n",
      "Input->Hidden Biases shape: (20,)\n",
      "Hidden->Output Weights shape: (20, 10)\n",
      "Hidden->Output Biases shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize MLP Parameters ---\n",
    "\n",
    "# Define network architecture\n",
    "input_size = 784\n",
    "hidden_nodes = 20\n",
    "output_size = 10\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize weights and biases for both layers\n",
    "# 1. Input to Hidden Layer\n",
    "weights_h = np.random.rand(input_size, hidden_nodes) * 0.1\n",
    "biases_h = np.random.rand(hidden_nodes) * 0.1\n",
    "\n",
    "# 2. Hidden to Output Layer\n",
    "weights_o = np.random.rand(hidden_nodes, output_size) * 0.1\n",
    "biases_o = np.random.rand(output_size) * 0.1\n",
    "\n",
    "# Print the shapes of our new parameters to verify\n",
    "print(\"--- MLP Parameters ---\")\n",
    "print(\"Input->Hidden Weights shape:\", weights_h.shape)\n",
    "print(\"Input->Hidden Biases shape:\", biases_h.shape)\n",
    "print(\"Hidden->Output Weights shape:\", weights_o.shape)\n",
    "print(\"Hidden->Output Biases shape:\", biases_o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on small 6000 sample subset for initial runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/10 ---\n",
      "Training Accuracy: 10.13%\n",
      "--- Epoch 2/10 ---\n",
      "Training Accuracy: 20.72%\n",
      "--- Epoch 3/10 ---\n",
      "Training Accuracy: 51.87%\n",
      "--- Epoch 4/10 ---\n",
      "Training Accuracy: 72.02%\n",
      "--- Epoch 5/10 ---\n",
      "Training Accuracy: 76.38%\n",
      "--- Epoch 6/10 ---\n",
      "Training Accuracy: 81.02%\n",
      "--- Epoch 7/10 ---\n",
      "Training Accuracy: 86.10%\n",
      "--- Epoch 8/10 ---\n",
      "Training Accuracy: 87.72%\n",
      "--- Epoch 9/10 ---\n",
      "Training Accuracy: 89.18%\n",
      "--- Epoch 10/10 ---\n",
      "Training Accuracy: 90.07%\n"
     ]
    }
   ],
   "source": [
    "# --- MLP Training Loop ---\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "train_images_small = train_images_flat[:6000]\n",
    "train_labels_small = train_labels_one_hot[:6000]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"--- Epoch {epoch + 1}/{epochs} ---\")\n",
    "    \n",
    "    # Loop through each image (x) and its one-hot label (d)\n",
    "    for x, d in zip(train_images_small, train_labels_small):\n",
    "        \n",
    "        # 1. FORWARD PASS\n",
    "        # Step 1.1: From Input to Hidden Layer\n",
    "        hidden_layer_input = np.dot(x, weights_h) + biases_h\n",
    "        hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "        \n",
    "        # Step 1.2: From Hidden to Output Layer\n",
    "        final_layer_input = np.dot(hidden_layer_output, weights_o) + biases_o\n",
    "        final_output = sigmoid(final_layer_input)\n",
    "\n",
    "        # 2. BACKWARD PASS (Backpropagation)\n",
    "        \n",
    "        # Step 2.1: Calculate the error term (delta) for the OUTPUT layer\n",
    "        delta_o = (d - final_output) * final_output * (1 - final_output)\n",
    "        \n",
    "        # Step 2.2: Calculate the error term for the HIDDEN layer\n",
    "        delta_h = np.dot(delta_o, weights_o.T) * hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "        # 3. UPDATE PARAMETERS\n",
    "        \n",
    "        # Step 3.1: Update Output Layer weights and biases\n",
    "        weights_o += learning_rate * np.outer(hidden_layer_output, delta_o)\n",
    "        biases_o += learning_rate * delta_o\n",
    "        \n",
    "        # Step 3.2: Update Hidden Layer weights and biases\n",
    "        weights_h += learning_rate * np.outer(x, delta_h)\n",
    "        biases_h += learning_rate * delta_h\n",
    "\n",
    "    # 4. TRACK PERFORMANCE (at the end of each epoch)\n",
    "    # Perform a full forward pass on the small training set to get the accuracy\n",
    "    hidden_output = sigmoid(np.dot(train_images_small, weights_h) + biases_h)\n",
    "    final_output = sigmoid(np.dot(hidden_output, weights_o) + biases_o)\n",
    "    \n",
    "    predicted_classes = np.argmax(final_output, axis=1)\n",
    "    true_classes = np.argmax(train_labels_small, axis=1)\n",
    "    \n",
    "    accuracy = np.mean(predicted_classes == true_classes)\n",
    "    print(f\"Training Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MLP Test Accuracy: 86.19%\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate the MLP on the Test Set ---\n",
    "\n",
    "# Perform a full forward pass on the test data using the final trained parameters\n",
    "# Step 1: Input to Hidden Layer\n",
    "hidden_output_test = sigmoid(np.dot(test_images_flat, weights_h) + biases_h)\n",
    "\n",
    "# Step 2: Hidden to Output Layer\n",
    "final_output_test = sigmoid(np.dot(hidden_output_test, weights_o) + biases_o)\n",
    "\n",
    "# Get the predicted class for each test image\n",
    "test_predicted_classes = np.argmax(final_output_test, axis=1)\n",
    "\n",
    "# Get the true class for each test image\n",
    "test_true_classes = np.argmax(test_labels_one_hot, axis=1)\n",
    "\n",
    "# Calculate the final test accuracy\n",
    "test_accuracy = np.mean(test_predicted_classes == test_true_classes)\n",
    "\n",
    "print(f\"Final MLP Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the full 60000 sample set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MLP Parameters ---\n",
      "Input->Hidden Weights shape: (784, 20)\n",
      "Input->Hidden Biases shape: (20,)\n",
      "Hidden->Output Weights shape: (20, 10)\n",
      "Hidden->Output Biases shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize MLP Parameters ---\n",
    "\n",
    "# Define network architecture\n",
    "input_size = 784\n",
    "hidden_nodes = 20\n",
    "output_size = 10\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize weights and biases for both layers\n",
    "# 1. Input to Hidden Layer\n",
    "weights_h = np.random.rand(input_size, hidden_nodes) * 0.1\n",
    "biases_h = np.random.rand(hidden_nodes) * 0.1\n",
    "\n",
    "# 2. Hidden to Output Layer\n",
    "weights_o = np.random.rand(hidden_nodes, output_size) * 0.1\n",
    "biases_o = np.random.rand(output_size) * 0.1\n",
    "\n",
    "# Print the shapes of our new parameters to verify\n",
    "print(\"--- MLP Parameters ---\")\n",
    "print(\"Input->Hidden Weights shape:\", weights_h.shape)\n",
    "print(\"Input->Hidden Biases shape:\", biases_h.shape)\n",
    "print(\"Hidden->Output Weights shape:\", weights_o.shape)\n",
    "print(\"Hidden->Output Biases shape:\", biases_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/10 ---\n",
      "Training Accuracy: 86.02%\n",
      "--- Epoch 2/10 ---\n",
      "Training Accuracy: 89.97%\n",
      "--- Epoch 3/10 ---\n",
      "Training Accuracy: 90.42%\n",
      "--- Epoch 4/10 ---\n",
      "Training Accuracy: 91.44%\n",
      "--- Epoch 5/10 ---\n",
      "Training Accuracy: 91.86%\n",
      "--- Epoch 6/10 ---\n",
      "Training Accuracy: 92.31%\n",
      "--- Epoch 7/10 ---\n",
      "Training Accuracy: 92.58%\n",
      "--- Epoch 8/10 ---\n",
      "Training Accuracy: 92.72%\n",
      "--- Epoch 9/10 ---\n",
      "Training Accuracy: 92.74%\n",
      "--- Epoch 10/10 ---\n",
      "Training Accuracy: 92.81%\n"
     ]
    }
   ],
   "source": [
    "# --- MLP Training Loop ---\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"--- Epoch {epoch + 1}/{epochs} ---\")\n",
    "    \n",
    "    # Loop through each image (x) and its one-hot label (d)\n",
    "    for x, d in zip(train_images_flat, train_labels_one_hot):\n",
    "        \n",
    "        # 1. FORWARD PASS\n",
    "        # Step 1.1: From Input to Hidden Layer\n",
    "        hidden_layer_input = np.dot(x, weights_h) + biases_h\n",
    "        hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "        \n",
    "        # Step 1.2: From Hidden to Output Layer\n",
    "        final_layer_input = np.dot(hidden_layer_output, weights_o) + biases_o\n",
    "        final_output = sigmoid(final_layer_input)\n",
    "\n",
    "        # 2. BACKWARD PASS (Backpropagation)\n",
    "        \n",
    "        # Step 2.1: Calculate the error term (delta) for the OUTPUT layer\n",
    "        delta_o = (d - final_output) * final_output * (1 - final_output)\n",
    "        \n",
    "        # Step 2.2: Calculate the error term for the HIDDEN layer\n",
    "        delta_h = np.dot(delta_o, weights_o.T) * hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "        # 3. UPDATE PARAMETERS\n",
    "        \n",
    "        # Step 3.1: Update Output Layer weights and biases\n",
    "        weights_o += learning_rate * np.outer(hidden_layer_output, delta_o)\n",
    "        biases_o += learning_rate * delta_o\n",
    "        \n",
    "        # Step 3.2: Update Hidden Layer weights and biases\n",
    "        weights_h += learning_rate * np.outer(x, delta_h)\n",
    "        biases_h += learning_rate * delta_h\n",
    "\n",
    "    # 4. TRACK PERFORMANCE (at the end of each epoch)\n",
    "    # Perform a full forward pass on the small training set to get the accuracy\n",
    "    hidden_output = sigmoid(np.dot(train_images_flat, weights_h) + biases_h)\n",
    "    final_output = sigmoid(np.dot(hidden_output, weights_o) + biases_o)\n",
    "    \n",
    "    predicted_classes = np.argmax(final_output, axis=1)\n",
    "    true_classes = np.argmax(train_labels_one_hot, axis=1)\n",
    "    \n",
    "    accuracy = np.mean(predicted_classes == true_classes)\n",
    "    print(f\"Training Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MLP Test Accuracy: 91.97%\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate the MLP on the Test Set ---\n",
    "\n",
    "# Perform a full forward pass on the test data using the final trained parameters\n",
    "# Step 1: Input to Hidden Layer\n",
    "hidden_output_test = sigmoid(np.dot(test_images_flat, weights_h) + biases_h)\n",
    "\n",
    "# Step 2: Hidden to Output Layer\n",
    "final_output_test = sigmoid(np.dot(hidden_output_test, weights_o) + biases_o)\n",
    "\n",
    "# Get the predicted class for each test image\n",
    "test_predicted_classes = np.argmax(final_output_test, axis=1)\n",
    "\n",
    "# Get the true class for each test image\n",
    "test_true_classes = np.argmax(test_labels_one_hot, axis=1)\n",
    "\n",
    "# Calculate the final test accuracy\n",
    "test_accuracy = np.mean(test_predicted_classes == test_true_classes)\n",
    "\n",
    "print(f\"Final MLP Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern_recognition_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
